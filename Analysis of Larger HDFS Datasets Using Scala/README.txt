Hello,

There are two folder's with two seprate job builds, one for each database used.

There's nothing special needed to run these if just doing a normal spark job. If you're using the spark shell be sure to look at the "notes.txt" file for the primary database as there are special things for that one

Also, you can find the datasets we used in the "Findings Report", not putting them in the git repo as they are ~180GB combined

Spark 2.4.2 and Hadoop 2.7 were used for this 